import torch
import torch.nn as nn
from torch.distributions.normal import Normal


'''Affine Parameter generator
    Based on one of the variables in the input, this function generates the 
    parameters of the affine transformation for the other variable.
            ___
    x ---> | m | ---> theta; theta = {t, log(s)}
            ¯¯¯
    In this case, theta comprises the translation and the log of the scale for the affine transformation
'''
class AffineParams2D(nn.Module):
    def __init__(self, input_size=1,hidden_size=64, num_hidden_layers=3, output_size=2):
        super(AffineParams2D, self).__init__()
        layers = [nn.Linear(input_size, hidden_size)]
        for _ in range(num_hidden_layers - 1):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(hidden_size, output_size))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        x = x.view(-1,1)
        return self.layers(x)

'''Affine Transform
    The coupling layer applied an affine transformation to a part of an input, given the
    parameters generated by another part of the input:
               ________
    x  -----> | Affine | ----> s*x + t
               ¯¯¯¯¯¯¯¯
    Where "s" and "t" are scale and translation, respectively.
'''
class AffineTransform(nn.Module):
    def __init__(self):
        super(AffineTransform, self).__init__()
        self.params = AffineParams2D()

    def forward(self, x, condition):
        x = x.view(-1,1)
        t, log_s = torch.chunk(self.params(condition), 2, dim=1)
        s = log_s.exp()
        z = s*x + t
        dz_by_dx = log_s
        return z, dz_by_dx

'''Coupling Layer
    Computes the transformation of one output, conditioned by the other output:
                 ________
    x2  ------> | Affine | ---> z2
                 ¯¯¯¯¯¯¯¯
                  ___^
             ___ |
            | m |
         __^ ¯¯¯  
        |       
    x1  ----------------------> z1 
    Here, the function "m" is AffineParams2D, and "Affine" is AffineTransform. 
'''
class CouplingLayer(nn.Module):
    def __init__(self, b_size, permute=False):
        super(CouplingLayer, self).__init__()
        self.affine = AffineTransform()
        self.permute = permute
        self.unit_jacobian = nn.Parameter(torch.ones(b_size, 1), requires_grad=False)

    def forward(self, x):
        x1, x2 = torch.chunk(x, 2, dim=1)
        if self.permute:
            z1, dz1_by_dx1 = self.affine(x1, condition=x2)
            z2, dz2_by_dx2 = x2, self.unit_jacobian
        else:
            z1, dz1_by_dx1 = x1, self.unit_jacobian
            z2, dz2_by_dx2 = self.affine(x2, condition=x1)
        z = torch.cat([z1, z2], dim=1)
        dz_by_dx = torch.cat([dz1_by_dx1, dz2_by_dx2], dim=1).prod(dim=1).unsqueeze(1)
        #dz_by_dz =
        return z, dz_by_dx

'''Flow2D
    Executes a composition of n coupling layers to transform input x into z:
            ____   z1   ____   z2      zn-1  ____
    x ---> | f1 | ---> | f2 | ---> ... ---> | fn | ---> z 
            ¯¯¯¯        ¯¯¯¯                 ¯¯¯¯
'''
class Flow2d(nn.Module):
    def __init__(self, b_size, n_layers = 3):
        super(Flow2d, self).__init__()
        self.model = nn.ModuleList()
        #self.register_buffer('sum_log_dz_by_dx', torch.zeros([0.,0.], requires_grad=False).unsqueeze(1))
        for n in range(n_layers):
            if n%2 == 0:
                self.model.append(CouplingLayer(b_size, permute=False))
            else:
                self.model.append(CouplingLayer(b_size, permute=True))

    def forward(self, x):
        z = x
        jacobians = []
        for flow in self.model:
            z, log_dz_by_dx = flow(z)
            jacobians.append(log_dz_by_dx)
        return z, torch.stack(jacobians).sum(0)
